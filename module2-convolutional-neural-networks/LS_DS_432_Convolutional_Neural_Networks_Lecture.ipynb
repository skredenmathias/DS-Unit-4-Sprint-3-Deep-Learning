{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (Prepare)\n",
    "\n",
    "> Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. *Goodfellow, et al.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe convolution and pooling\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a convolutional neural network to a classification task\n",
    "- <a href=\"#p3\">Part 3: </a>Use a pre-trained convolution neural network for image classification\n",
    "\n",
    "Modern __computer vision__ approaches rely heavily on convolutions as both a dimensionality reduction and feature extraction method. Before we dive into convolutions, let's talk about some of the common computer vision applications: \n",
    "* Classification [(Hot Dog or Not Dog)](https://www.youtube.com/watch?v=ACmydtFDTGs)\n",
    "* Object Detection [(YOLO)](https://www.youtube.com/watch?v=MPU2HistivI)\n",
    "* Pose Estimation [(PoseNet)](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)\n",
    "* Facial Recognition [Emotion Detection](https://www.cbronline.com/wp-content/uploads/2018/05/Mona-lIsa-test-570x300.jpg)\n",
    "* and *countless* more \n",
    "\n",
    "We are going to focus on classification and pre-trained classification today. What are some of the applications of image classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('MPU2HistivI', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Convolution & Pooling (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Like neural networks themselves, CNNs are inspired by biology - specifically, the receptive fields of the visual cortex.\n",
    "\n",
    "Put roughly, in a real brain the neurons in the visual cortex *specialize* to be receptive to certain regions, shapes, colors, orientations, and other common visual features. In a sense, the very structure of our cognitive system transforms raw visual input, and sends it to neurons that specialize in handling particular subsets of it.\n",
    "\n",
    "CNNs imitate this approach by applying a convolution. A convolution is an operation on two functions that produces a third function, showing how one function modifies another. Convolutions have a [variety of nice mathematical properties](https://en.wikipedia.org/wiki/Convolution#Properties) - commutativity, associativity, distributivity, and more. Applying a convolution effectively transforms the \"shape\" of the input.\n",
    "\n",
    "One common confusion - the term \"convolution\" is used to refer to both the process of computing the third (joint) function and the process of applying it. In our context, it's more useful to think of it as an application, again loosely analogous to the mapping from visual field to receptive areas of the cortex in a real animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('IOHayh06LJ4', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's try to do some convolutions and pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "Consider blurring an image - assume the image is represented as a matrix of numbers, where each number corresponds to the color value of a pixel.\n",
    "\n",
    "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.27.17+AM.png)\n",
    "\n",
    "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
    "\n",
    "\n",
    "Helpful Terms:\n",
    "- __Filter__: The weights (parameters) we will apply to our input image.\n",
    "- __Stride__: How the filter moves across the image\n",
    "- __Padding__: Zeros (or other values) around the  the input image border (kind of like a frame of zeros). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "OsAcbKvoeaqU",
    "outputId": "dbb28705-36c7-4691-f7df-e9f82e3ee91e"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color, io\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "austen = io.imread('https://dl.airtable.com/S1InFmIhQBypHBL0BICi_austen.jpg')\n",
    "austen_grayscale = rescale_intensity(color.rgb2gray(austen))\n",
    "austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "KN-ibr_DhyaV",
    "outputId": "241716ac-3415-4cfd-9602-0dd59a80ed47"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_grayscale, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "QopB0uo6lNxq",
    "outputId": "2364bf3d-8fb9-487a-d2db-eb794939c77a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "\n",
    "horizontal_edge_convolution = np.array([[1,1,1,1,1],\n",
    "                                        [0,0,0,0,0],\n",
    "                                        [-1,-1,-1,-1,-1]])\n",
    "\n",
    "vertical_edge_convolution = np.array([[1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1],\n",
    "                                     [1, 0, 0, 0, -1]])\n",
    "\n",
    "austen_edges = nd.convolve(austen_grayscale, horizontal_edge_convolution)\n",
    "austen_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "-LwEpFW1l-6b",
    "outputId": "51b9bdf4-dab6-406a-f98b-fd0a7b480859"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_edges, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layer\n",
    "\n",
    "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.26.13+AM.png)\n",
    "\n",
    "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
    "\n",
    "We use Pooling Layers to reduce the dimensionality of the feature maps. We get smaller and smaller feature set by apply convolutions and then pooling layers. \n",
    "\n",
    "Let's take a look very simple example using Austen's pic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import block_reduce\n",
    "\n",
    "reduced = block_reduce(austen_grayscale, (2,2), np.max)\n",
    "plt.imshow(reduced, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to be able to describe convolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs for Classification (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOep4ugw8coa"
   },
   "source": [
    "### Typical CNN Architecture\n",
    "\n",
    "![A Typical CNN](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/800px-Typical_cnn.png)\n",
    "\n",
    "The first stage of a CNN is, unsurprisingly, a convolution - specifically, a transformation that maps regions of the input image to neurons responsible for receiving them. The convolutional layer can be visualized as follows:\n",
    "\n",
    "![Convolutional layer](https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png)\n",
    "\n",
    "The red represents the original input image, and the blue the neurons that correspond.\n",
    "\n",
    "As shown in the first image, a CNN can have multiple rounds of convolutions, [downsampling](https://en.wikipedia.org/wiki/Downsampling_(signal_processing)) (a digital signal processing technique that effectively reduces the information by passing through a filter), and then eventually a fully connected neural network and output layer. Typical output layers for a CNN would be oriented towards classification or detection problems - e.g. \"does this picture contain a cat, a dog, or some other animal?\"\n",
    "\n",
    "\n",
    "#### A Convolution in Action\n",
    "\n",
    "![Convolution](https://miro.medium.com/max/1170/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n",
    "\n",
    "\n",
    "\n",
    "Why are CNNs so popular?\n",
    "1. Compared to prior image learning techniques, they require relatively little image preprocessing (cropping/centering, normalizing, etc.)\n",
    "2. Relatedly, they are *robust* to all sorts of common problems in images (shifts, lighting, etc.)\n",
    "\n",
    "Actually training a cutting edge image classification CNN is nontrivial computationally - the good news is, with transfer learning, we can get one \"off-the-shelf\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential, Model # <- May Use\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32*32*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will apply CNNs to a classification task in the module project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Image Classification (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ic_wzFnprwXI"
   },
   "source": [
    "### Transfer Learning Repositories\n",
    "\n",
    "#### TensorFlow Hub\n",
    "\n",
    "\"A library for reusable machine learning modules\"\n",
    "\n",
    "This lets you quickly take advantage of a model that was trained with thousands of GPU hours. It also enables transfer learning - reusing a part of a trained model (called a module) that includes weights and assets, but also training the overall model some yourself with your own data. The advantages are fairly clear - you can use less training data, have faster training, and have a model that generalizes better.\n",
    "\n",
    "https://www.tensorflow.org/hub/\n",
    "\n",
    "TensorFlow Hub is very bleeding edge, and while there's a good amount of documentation out there, it's not always updated or consistent. You'll have to use your problem-solving skills if you want to use it!\n",
    "\n",
    "#### Keras API - Applications\n",
    "\n",
    "> Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n",
    "\n",
    "There is a decent selection of important benchmark models. We'll focus on an image classifier: ResNet50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FM_ApKbGYM9S",
    "outputId": "4bfd7ce4-47e5-4320-d1b8-2b20e9f66416"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def process_img_path(img_path):\n",
    "  return image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "def img_contains_banana(img):\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  model = ResNet50(weights='imagenet')\n",
    "  features = model.predict(x)\n",
    "  results = decode_predictions(features, top=3)[0]\n",
    "  print(results)\n",
    "  for entry in results:\n",
    "    if entry[1] == 'banana':\n",
    "      return entry[2]\n",
    "  return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "_cQ8ZsJF_Z3B",
    "outputId": "02545656-8773-4bb2-9ff5-36d8c658dc00"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "image_urls = [\"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/negative_examples/example11.jpeg\",\n",
    "              \"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/positive_examples/example0.jpeg\"]\n",
    "\n",
    "for _id,img in enumerate(image_urls): \n",
    "    r = requests.get(img)\n",
    "    with open(f'example{_id}.jpg', 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "Gxzkai0q_d-4",
    "outputId": "a6bd9b95-9665-4df0-c74d-3d4e876eaf48"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./example0.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "X8NIlClb_n8s",
    "outputId": "7c9b9f98-073e-4ab0-a336-e3fc89fa8439"
   },
   "outputs": [],
   "source": [
    "img_contains_banana(process_img_path('example0.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "YIwtRazQ_tQr",
    "outputId": "7be6599b-253d-4600-e1f5-ac0ab0f2dfbc"
   },
   "outputs": [],
   "source": [
    "Image(filename='example1.jpg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "GDXwkPWOAB14",
    "outputId": "6493a0cb-b57b-43be-8a4e-ac06e51bdada"
   },
   "outputs": [],
   "source": [
    "img_contains_banana(process_img_path('example1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdF5A88oPYvX"
   },
   "source": [
    "Notice that, while it gets it right, the confidence for the banana image is fairly low. That's because so much of the image is \"not-banana\"! How can this be improved? Bounding boxes to center on items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to apply a pretrained model to a classificaiton problem today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe convolution and pooling\n",
    "    * A Convolution is a function applied to another function to produce a third function\n",
    "    * Convolutional Kernels are typically 'learned' during the process of training a Convolution Neural Network\n",
    "    * Pooling is a dimensionality reduction technique that uses either Max or Average of a feature map region to downsample data\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a convolutional neural network to a classification task\n",
    "    * Keras has layers for convolutions :) \n",
    "- <a href=\"#p3\">Part 3: </a>Transfer Learning for Image Classification\n",
    "    * Check out both pretinaed models available in Keras & TensorFlow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- *_Deep Learning_*. Goodfellow *et al.*\n",
    "- *Hands-on Machine Learnign with Scikit-Learn, Keras & Tensorflow*\n",
    "- [Keras CNN Tutorial](https://www.tensorflow.org/tutorials/images/cnn)\n",
    "- [Tensorflow + Keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
    "- [Convolution Wiki](https://en.wikipedia.org/wiki/Convolution)\n",
    "- [Keras Conv2D: Working with CNN 2D Convolutions in Keras](https://missinglink.ai/guides/keras/keras-conv2d-working-cnn-2d-convolutions-keras/)\n",
    "- [Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "- [A Beginner's Guide to Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NNF-DS10",
   "language": "python",
   "name": "u4-s2-nnf-ds10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
